{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig():\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 12\n",
    "    d_model: int = 768\n",
    "    vocab_size: int = 50257\n",
    "    window_size: int = 1024\n",
    "    dropout: int = 0.1\n",
    "    batch_size: int = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.c_attn = nn.Linear(config.d_model, 3*config.d_model)\n",
    "        self.c_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(config.window_size, config.window_size)).view(\n",
    "                1, 1, config.window_size, config.window_size\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # NOTE Residual weight scaling at initialization\n",
    "        self.RESID_SCALING = 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        qkv = self.c_attn(x)           # (B, T, 3*C)\n",
    "        q, k, v = qkv.split(C, dim=2)  # (B, T, C)\n",
    "\n",
    "        q = q.view(B, T, self.config.n_heads, C // self.config.n_heads).transpose(1,2) # (B, nh, T, d_k)\n",
    "        k = k.view(B, T, self.config.n_heads, C // self.config.n_heads).transpose(1,2) # (B, nh, T, d_k)\n",
    "        v = v.view(B, T, self.config.n_heads, C // self.config.n_heads).transpose(1,2) # (B, nh, T, d_v)\n",
    "\n",
    "        attn = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(C // self.config.n_heads)                 # (B, nh, T, T)\n",
    "        masked_attn = attn.masked_fill(self.mask[:,:,:T,:T]==0, float(\"-inf\"))     # (B, nh, T, T)\n",
    "        attn_weights = F.softmax(masked_attn, dim=-1)               # (B, nh, T, T)\n",
    "        # apply dropout\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        context = torch.matmul(attn_weights, v)                       # (B, nh, T, d_v)\n",
    "        context = context.transpose(1,2).contiguous().view(B, T, C)   # (B, T, C)\n",
    "        context = self.c_proj(context)                                # (B, T, C)\n",
    "        context = self.resid_dropout(context)                         # (B, T, C)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_fc = nn.Linear(config.d_model, 4 * config.d_model)\n",
    "        self.c_proj = nn.Linear(4*config.d_model, config.d_model)\n",
    "        self.act = nn.GELU(approximate='tanh')    # NOTE pay attention to how to use\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # NOTE Residual weight scaling at initialization\n",
    "        self.RESID_SCALING = 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.d_model)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.d_model)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.d_model),\n",
    "            wpe = nn.Embedding(config.window_size, config.d_model),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([GPT2Block(config) for _ in range(config.n_layers)]),\n",
    "            ln_f = nn.LayerNorm(config.d_model),  # NOTE adidtional normalization\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        # NOTE Weight sharing used in the original transformer paper\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, \"RESID_SCALING\"):\n",
    "                std *= torch.sqrt(2 * self.config.n_layers) \n",
    "            nn.init.normal_(module, mean=0.0, std=std)\n",
    "            if module.bias:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "    \n",
    "    def forward(self, inp, target=None):\n",
    "        \n",
    "        B, T = inp.shape\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=x.device)\n",
    "        pe = self.transformer.wpe(pos)\n",
    "        te = self.transformer.wte(inp)\n",
    "\n",
    "        x = te + pe\n",
    "        # NOTE here is the dropout\n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if target:\n",
    "            # NOTE pay attention to how to adapt the dimention of tensors\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='tanh')\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "model = GPT2(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a: int, b=1, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "# Example 1: Basic function signature\n",
    "def add(a: int, b=1, *args, **kwargs):\n",
    "    return a + b\n",
    "\n",
    "sig = inspect.signature(add)\n",
    "print(sig)  # (a, b=1, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "# Example 1: Basic function signature\n",
    "def add(a, b=1, *args, **kwargs):\n",
    "    return a + b\n",
    "\n",
    "sig = inspect.signature(add)\n",
    "print(sig)  # (a, b=1, *args, **kwargs)\n",
    "\n",
    "# Example 2: Getting parameter details\n",
    "for name, param in sig.parameters.items():\n",
    "    print(f\"{name}: {param.default}, {param.kind}\")\n",
    "\n",
    "# Example 3: Checking if a parameter exists\n",
    "has_b = 'b' in sig.parameters\n",
    "print(has_b)  # True\n",
    "\n",
    "# Example 4: Binding arguments to parameters\n",
    "bound_args = sig.bind(5, 3)\n",
    "print(bound_args.arguments)  # {'a': 5, 'b': 3}\n",
    "\n",
    "# Example 5: Class method signature\n",
    "class Calculator:\n",
    "    def multiply(self, x, y=2):\n",
    "        return x * y\n",
    "\n",
    "calc_sig = inspect.signature(Calculator.multiply)\n",
    "print(calc_sig)  # (self, x, y=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = model.state_dict()\n",
    "sd_keys = sd.keys()\n",
    "sd_keys = [k for k in sd_keys if not k.endswith('.attn.mask')] # discard this mask / buffer, not a param\n",
    "len(sd_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(\"/data/repos/huggingface/gpt2\")\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "sd_keys_hf = sd_hf.keys()\n",
    "sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sd_keys_hf:\n",
    "    if any(k.endswith(w) for w in transposed):\n",
    "        # special treatment for the Conv1D weights we need to transpose\n",
    "        assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "        with torch.no_grad():\n",
    "            sd[k].copy_(sd_hf[k].t())\n",
    "    else:\n",
    "        # vanilla copy over the other parameters\n",
    "        assert sd_hf[k].shape == sd[k].shape\n",
    "        with torch.no_grad():\n",
    "            sd[k].copy_(sd_hf[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
